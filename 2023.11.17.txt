
1, TITLE: Digital Socrates: Evaluating LLMs Through Explanation Critiques
AUTHORS: Yuling Gu ; Oyvind Tafjord ; Peter Clark
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations.

2, TITLE: Large Language Model Inference with Lexical Shortlisting
AUTHORS: Nikolay Bogoychev ; Pinzhen Chen ; Barry Haddow ; Alexandra Birch
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Large language model (LLM) inference is computation and memory intensive, so we adapt lexical shortlisting to it hoping to improve both.

3, TITLE: When Large Language Models Contradict Humans? Large Language Models' Sycophantic Behaviour
AUTHORS: Leonardo Ranaldi ; Giulia Pucci
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: This phenomenon decreases the bias, robustness, and, consequently, their reliability. In this paper, we shed light on the suggestibility of LLMs to sycophantic behaviour, demonstrating these tendencies via human-influenced prompts over different tasks.

4, TITLE: From Scroll to Misbelief: Modeling The Unobservable Susceptibility to Misinformation on Social Media
AUTHORS: YANCHEN LIU et. al.
CATEGORY: cs.CL [cs.CL, cs.CY, cs.SI]
HIGHLIGHT: Existing susceptibility studies heavily rely on the self-reported beliefs, making any downstream applications on susceptability hard to scale. To address these limitations, in this work, we propose a computational model to infer users' susceptibility levels given their activities.

5, TITLE: Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals
AUTHORS: YANAI ELAZAR et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Are these correlations picked up by models trained on the full input data? To address this question, we propose a new evaluation method, Counterfactual Attentiveness Test (CAT).

6, TITLE: Regularized Conventions: Equilibrium Computation As A Model of Pragmatic Reasoning
AUTHORS: Athul Paul Jacob ; Gabriele Farina ; Jacob Andreas
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We present a model of pragmatic language understanding, where utterances are produced and understood by searching for regularized equilibria of signaling games.

7, TITLE: On Evaluating The Integration of Reasoning and Action in LLM Agents with Database Question Answering
AUTHORS: LINYONG NAN et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations.

8, TITLE: GEE! Grammar Error Explanation with Large Language Models
AUTHORS: Yixiao Song ; Kalpesh Krishna ; Rajesh Bhatt ; Kevin Gimpel ; Mohit Iyyer
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Such explanations are essential for helping users learn the language by gaining a deeper understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006). To address this gap, we propose the task of grammar error explanation, where a system needs to provide one-sentence explanations for each grammatical error in a pair of erroneous and corrected sentences.

9, TITLE: One Size Does Not Fit All: Customizing Open-Domain Procedures
AUTHORS: YASH KUMAR LAL et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We introduce CustomPlans, a probe dataset of customization hints that encodes diverse user needs for open-domain How-to procedures.

10, TITLE: Program-Aided Reasoners (better) Know What They Know
AUTHORS: ANUBHA KABRA et. al.
CATEGORY: cs.AI [cs.AI]
HIGHLIGHT: In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models.

11, TITLE: Effective Large Language Model Adaptation for Improved Grounding
AUTHORS: Xi Ye ; Ruoxi Sun ; Sercan �. Arik ; Tomas Pfister
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: To effectively enable this capability, we tune LLMs to ground the claims in their responses to retrieved documents by providing citations. This tuning on top of the pre-trained LLMs requires a small amount of data that needs to be constructed in a particular way to learn the grounding information, for which we introduce a data construction method.

12, TITLE: Towards Robust Temporal Reasoning of Large Language Models Via A Multi-Hop QA Dataset and Pseudo-Instruction Tuning
AUTHORS: Qingyu Tan ; Hwee Tou Ng ; Lidong Bing
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this paper, we propose a complex temporal question-answering (QA) dataset Complex-TR that focuses on multi-answer and multi-hop temporal reasoning.

13, TITLE: BLT: Can Large Language Models Handle Basic Legal Text?
AUTHORS: Andrew Blair-Stanek ; Nils Holzenberger ; Benjamin Van Durme
CATEGORY: cs.CL [cs.CL, cs.AI, I.2.1; I.2.7; J.7]
HIGHLIGHT: We find that the best publicly available LLMs like GPT-4 and PaLM 2 currently perform poorly at basic text handling required of lawyers or paralegals, such as looking up the text at a line of a witness deposition or at a subsection of a contract. We introduce a benchmark to quantify this poor performance, which casts into doubt LLMs' current reliability as-is for legal practice.

14, TITLE: Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization
AUTHORS: ALEXANDRA CHRONOPOULOU et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this paper, we propose to improve zero-shot cross-lingual transfer by composing language or task specialized parameters.

15, TITLE: Large Language Models Are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition
AUTHORS: Tariq Alhindi ; Smaranda Muresan ; Preslav Nakov
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this study, we aim to enhance existing models for fallacy recognition by incorporating additional context and by leveraging large language models to generate synthetic data, thus increasing the representation of the infrequent classes.

16, TITLE: JAB: Joint Adversarial Prompting and Belief Augmentation
AUTHORS: NINAREH MEHRABI et. al.
CATEGORY: cs.AI [cs.AI, cs.CL]
HIGHLIGHT: Here we introduce a joint framework in which we simultaneously probe and improve the robustness of a black-box target model via adversarial prompting and belief augmentation using iterative feedback loops.

17, TITLE: PixT3: Pixel-based Table To Text Generation
AUTHORS: I�igo Alonso ; Eneko Agirre ; Mirella Lapata
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this paper, we demonstrate that image representation of tables are more space-efficient than the typical textual linearizations, and multi-modal approaches are competitive in Table-to-Text tasks.

18, TITLE: What Constitutes A Faithful Summary? Preserving Author Perspectives in News Summarization
AUTHORS: YUHAN LIU et. al.
CATEGORY: cs.CL [cs.CL, cs.LG]
HIGHLIGHT: In this work, we take a first step towards designing summarization systems that are faithful to the author's opinions and perspectives.

19, TITLE: Enhancing Medical Text Evaluation with GPT-4
AUTHORS: YIQING XIE et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Existing evaluation metrics either focus on coarse-level evaluation that assigns one score for the whole generated output or rely on evaluation models trained on general domain, resulting in inaccuracies when adapted to the medical domain. To address these issues, we propose a set of factuality-centric evaluation aspects and design corresponding GPT-4-based metrics for medical text generation.

20, TITLE: Performance Trade-offs of Watermarking Large Language Models
AUTHORS: Anirudh Ajith ; Sameer Singh ; Danish Pruthi
CATEGORY: cs.CL [cs.CL, cs.AI, cs.CR, cs.LG]
HIGHLIGHT: In this work, we evaluate the performance of watermarked LLMs on a diverse suite of tasks, including text classification, textual entailment, reasoning, question answering, translation, summarization, and language modeling.

21, TITLE: ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems
AUTHORS: Jon Saad-Falcon ; Omar Khattab ; Christopher Potts ; Matei Zaharia
CATEGORY: cs.CL [cs.CL, cs.AI, cs.IR]
HIGHLIGHT: We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance.

22, TITLE: Divergences Between Language Models and Human Brains
AUTHORS: Yuchen Zhou ; Emmy Liu ; Graham Neubig ; Leila Wehbe
CATEGORY: cs.CL [cs.CL, cs.AI, cs.LG, q-bio.NC]
HIGHLIGHT: Despite this, there is little work exploring systematic differences between human and machine language processing using brain data. To address this question, we examine the differences between LM representations and the human brain's responses to language, specifically by examining a dataset of Magnetoencephalography (MEG) responses to a written narrative.

23, TITLE: Crafting In-context Examples According to LMs' Parametric Knowledge
AUTHORS: Yoonsang Lee ; Pranav Atreya ; Xi Ye ; Eunsol Choi
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We perform analysis on three multi-answer question answering datasets, which allows us to further study answer set ordering strategies based on the LM's knowledge about each answer.

24, TITLE: Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs
AUTHORS: Michael J. Q. Zhang ; Eunsol Choi
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Resolving ambiguities through interaction is a hallmark of natural language, and modeling this behavior is a core challenge in crafting AI assistants. In this work, we study such behavior in LMs by proposing a task-agnostic framework for resolving ambiguity by asking users clarifying questions.

25, TITLE: Characterizing Tradeoffs in Language Model Decoding with Informational Interpretations
AUTHORS: Chung-Ching Chang ; William W. Cohen ; Yun-Hsuan Sung
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We propose a theoretical framework for formulating language model decoder algorithms with dynamic programming and information theory.

26, TITLE: Outcome-supervised Verifiers for Planning in Mathematical Reasoning
AUTHORS: Fei Yu ; Anningzhe Gao ; Benyou Wang
CATEGORY: cs.AI [cs.AI, cs.CL]
HIGHLIGHT: By rethinking this approach, we argue that assessing potentials of incomplete reasoning paths could be more advantageous as it guides towards correct final answers, transforming the task into a \textit{planning} problem.

27, TITLE: HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs
AUTHORS: JUNYING CHEN et. al.
CATEGORY: cs.CL [cs.CL, cs.AI, cs.LG]
HIGHLIGHT: The challenge lies in the heterogeneity of data across the two training stages, as it varies in languages, genres, or formats. To tackle this and simplify the learning protocol, we propose to transform heterogeneous data, from the both pre-training and supervised stages, into a unified, simple input-output pair format.

28, TITLE: On The Exploitability of Reinforcement Learning with Human Feedback for Large Language Models
AUTHORS: Jiongxiao Wang ; Junlin Wu ; Muhao Chen ; Yevgeniy Vorobeychik ; Chaowei Xiao
CATEGORY: cs.AI [cs.AI, cs.CL, cs.CR, cs.HC]
HIGHLIGHT: To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates' selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost).

29, TITLE: Investigating The Emergent Audio Classification Ability of ASR Foundation Models
AUTHORS: Rao Ma ; Adian Liusie ; Mark J. F. Gales ; Kate M. Knill
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work we investigate the ability of Whisper and MMS, ASR foundation models trained primarily for speech recognition, to perform zero-shot audio classification.

30, TITLE: Pinpoint, Not Criticize: Refining Large Language Models Via Fine-Grained Actionable Feedback
AUTHORS: WENDA XU et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we propose an inference time optimization method FITO to use fine-grained actionable feedback in the form of error type, error location and severity level that are predicted by a learned error pinpoint model for iterative refinement.

31, TITLE: Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations
AUTHORS: WENJIE MO et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: This gap becomes particularly pronounced in the context of Large Language Models (LLMs) deployed as Web Services, which typically offer only black-box access, rendering training-time defenses impractical. To bridge this gap, our work introduces defensive demonstrations, an innovative backdoor defense strategy for blackbox large language models.

32, TITLE: Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking
AUTHORS: NAN XU et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs.

33, TITLE: Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go Without Hallucination?
AUTHORS: BANGZHENG LI et. al.
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path. To quantify this phenomenon, we propose a novel probing method and benchmark called EureQA.

34, TITLE: DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data
AUTHORS: YILUN ZHAO et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning and problem-solving capabilities of LLMs in the context of understanding and analyzing financial documents containing both text and tables.

35, TITLE: KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains
AUTHORS: YILUN ZHAO et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We introduce KnowledgeMath, a novel benchmark designed to evaluate LLMs' capabilities in applying financial knowledge to solve complex math word problems.

36, TITLE: Investigating Data Contamination in Modern Benchmarks for Large Language Models
AUTHORS: Chunyuan Deng ; Yilun Zhao ; Xiangru Tang ; Mark Gerstein ; Arman Cohan
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: In this paper we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs.

37, TITLE: Lexical Repetitions Lead to Rote Learning: Unveiling The Impact of Lexical Overlap in Train and Test Reference Summaries
AUTHORS: Prafulla Kumar Choubey ; Alexander R. Fabbri ; Caiming Xiong ; Chien-Sheng Wu
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We propose a fine-grained evaluation protocol by partitioning a test set based on the lexical similarity of reference test summaries with training summaries.

38, TITLE: Pachinko: Patching Interpretable QA Models Through Natural Language Feedback
AUTHORS: Chaitanya Malaviya ; Subin Lee ; Dan Roth ; Mark Yatskar
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We sample these rationales from large language models using few-shot prompting for two reading comprehension datasets, and then perform two user studies. In the first one, we present users with incorrect answers and corresponding rationales of various formats and ask them to provide natural language feedback to revise the rationale.

39, TITLE: A Reevaluation of Event Extraction: Past, Present, and Future Challenges
AUTHORS: KUAN-HAO HUANG et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we first identify and discuss these evaluation challenges, including the unfair comparisons resulting from different assumptions about data or different data preprocessing steps, the incompleteness of the current evaluation framework leading to potential dataset bias or data split bias, and low reproducibility of prior studies. To address these challenges, we propose TextEE, a standardized, fair, and reproducible benchmark for event extraction.

40, TITLE: X-Mark: Towards Lossless Watermarking Through Lexical Redundancy
AUTHORS: LIANG CHEN et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: However, existing methods can severely degrade text quality due to arbitrary vocabulary partitioning, which disrupts the language model's expressiveness and impedes textual coherence. To mitigate this, we introduce XMark, a novel approach that capitalizes on text redundancy within the lexical space.

41, TITLE: Leveraging Code to Improve In-context Learning for Semantic Parsing
AUTHORS: Ben Bogin ; Shivanshu Gupta ; Peter Clark ; Ashish Sabharwal
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we improve the effectiveness of ICL for semantic parsing by (1) using general-purpose programming languages such as Python instead of DSLs, and (2) augmenting prompts with a structured domain description that includes, e.g., the available classes and functions.

42, TITLE: How Trustworthy Are Open-Source LLMs? An Assessment Under Malicious Demonstrations Shows Their Vulnerabilities
AUTHORS: Lingbo Mo ; Boshi Wang ; Muhao Chen ; Huan Sun
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations.

43, TITLE: Large Language Models for Propaganda Span Annotation
AUTHORS: Maram Hasanain ; Fatema Ahmed ; Firoj Alam
CATEGORY: cs.CL [cs.CL, 68T50, F.2.2; I.2.7]
HIGHLIGHT: Significant research efforts have been devoted to the first two scenarios compared to the latter. Therefore, in this study, we focus on the task of detecting propagandistic textual spans.

44, TITLE: LLMs As Narcissistic Evaluators: When Ego Inflates Evaluation Scores
AUTHORS: Yiqi Liu ; Nafise Sadat Moosavi ; Chenghua Lin
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks.

45, TITLE: AfriMTE and AfriCOMET: Empowering COMET to Embrace Under-resourced African Languages
AUTHORS: JIAYI WANG et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Embedding-based metrics such as COMET correlate better; however, lack of evaluation data with human ratings for under-resourced languages, complexity of annotation guidelines like Multidimensional Quality Metrics (MQM), and limited language coverage of multilingual encoders have hampered their applicability to African languages. In this paper, we address these challenges by creating high-quality human evaluation data with a simplified MQM guideline for error-span annotation and direct assessment (DA) scoring for 13 typologically diverse African languages.

46, TITLE: Prompt Optimisation with Random Sampling
AUTHORS: Yao Lu ; Jiayi Wang ; Sebastian Riedel ; Pontus Stenetorp
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: We demonstrate that even randomly chosen tokens from the vocabulary as separators can achieve near-state-of-the-art performance. We analyse this phenomenon in detail using three different random generation strategies, establishing that the language space is rich with potential good separators, regardless of the underlying language model size.

47, TITLE: The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text
AUTHORS: Yanzhu Guo ; Guokan Shang ; Michalis Vazirgiannis ; Chlo� Clavel
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: This study investigates the consequences of training large language models (LLMs) on synthetic data generated by their predecessors, an increasingly prevalent practice aimed at addressing the limited supply of human-generated training data.

48, TITLE: Take One Step at A Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning
AUTHORS: Kazuma Hashimoto ; Karthik Raman ; Michael Bendersky
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Unlike the previous work, we introduce a novel labeling method, incremental utility, which estimates how much incremental knowledge is brought into the LLMs by a demonstration.

49, TITLE: Show Your Work with Confidence: Confidence Bands for Tuning Curves
AUTHORS: Nicholas Lourie ; Kyunghyun Cho ; He He
CATEGORY: cs.CL [cs.CL, cs.LG, stat.ML]
HIGHLIGHT: We present the first method to construct valid confidence bands for tuning curves.

50, TITLE: Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources
AUTHORS: YIPEI XU et. al.
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: In this paper, we identify the disadvantage of heterogeneous corpora from multiple sources for pre-training PLMs.

51, TITLE: Code Models Are Zero-shot Precondition Reasoners
AUTHORS: LAJANUGEN LOGESWARAN et. al.
CATEGORY: cs.AI [cs.AI]
HIGHLIGHT: Leveraging code representations, we extract action preconditions from demonstration trajectories in a zero-shot manner using pre-trained code models. Given these extracted preconditions, we propose a precondition-aware action sampling strategy that ensures actions predicted by a policy are consistent with preconditions.

52, TITLE: Interpreting User Requests in The Context of Natural Language Standing Instructions
AUTHORS: NIKITA MOGHE et. al.
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: Users of natural language interfaces, generally powered by Large Language Models (LLMs),often must repeat their preferences each time they make a similar request. To alleviate this, we propose including some of a user's preferences and instructions in natural language -- collectively termed standing instructions -- as additional context for such interfaces.

53, TITLE: Where Do People Tell Stories Online? Story Detection Across Online Communities
AUTHORS: Maria Antoniak ; Joel Mire ; Maarten Sap ; Elliott Ash ; Andrew Piper
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We introduce a codebook and create the Storytelling in Online Communities Corpus, an expert-annotated dataset of 502 English-language posts and comments with labeled story and event spans.

54, TITLE: Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks
AUTHORS: Huaman Sun ; Jiaxin Pei ; Minje Choi ; David Jurgens
CATEGORY: cs.CL [cs.CL, cs.AI, cs.CY, cs.HC, cs.LG]
HIGHLIGHT: In this study, leveraging the POPQUORN dataset which contains annotations of diverse demographic backgrounds, we conduct a series of experiments on four popular LLMs to investigate their capability to understand group differences and potential biases in their predictions for politeness and offensiveness.

55, TITLE: Tracking The Newsworthiness of Public Documents
AUTHORS: ALEXANDER SPANGHER et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work we focus on news coverage of local public policy in the San Francisco Bay Area by the San Francisco Chronicle.

56, TITLE: MacGyver: Are Large Language Models Creative Problem Solvers?
AUTHORS: YUFEI TIAN et. al.
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: We explore the creative problem-solving capabilities of modern large language models (LLMs) in a constrained setting.

57, TITLE: You Don't Need A Personality Test to Know These Models Are Unreliable: Assessing The Reliability of Large Language Models on Psychometric Instruments
AUTHORS: BANGZHAO SHU et. al.
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: In this study, we take a cautionary step back and examine whether the current format of prompting enables LLMs to provide responses in a consistent and robust manner.

58, TITLE: Is "A Helpful Assistant" The Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts
AUTHORS: Mingqian Zheng ; Jiaxin Pei ; David Jurgens
CATEGORY: cs.CL [cs.CL, cs.AI, cs.CY, cs.HC, cs.LG]
HIGHLIGHT: In this study, we present a systematic evaluation of how social roles in system prompts affect model performance.

59, TITLE: PsyBench: A Balanced and In-depth Psychological Chinese Evaluation Benchmark for Foundation Models
AUTHORS: JUNLEI ZHANG et. al.
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: This shortfall has led to skewed results, given that LLMs exhibit varying performance across different subjects and knowledge areas. To address this issue, we present psybench, the first comprehensive Chinese evaluation suite that covers all the necessary knowledge required for graduate entrance exams.

60, TITLE: Improving The Generation Quality of Watermarked Large Language Models Via Word Importance Scoring
AUTHORS: Yuhang Li ; Yihan Wang ; Zhouxing Shi ; Cho-Jui Hsieh
CATEGORY: cs.CL [cs.CL, cs.CR, cs.LG]
HIGHLIGHT: In this work, we propose to improve the quality of texts generated by a watermarked language model by Watermarking with Importance Scoring (WIS).

61, TITLE: How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!
AUTHORS: Shirley Anugrah Hayati ; Minhwa Lee ; Dheeraj Rajagopal ; Dongyeop Kang
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts.

62, TITLE: Which Modality Should I Use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models
AUTHORS: Debarati Das ; Ishaan Gupta ; Jaideep Srivastava ; Dongyeop Kang
CATEGORY: cs.CL [cs.CL, cs.SI]
HIGHLIGHT: This paper explores how to better integrate graph data with LLMs and presents a novel approach using various encoding modalities (e.g., text, image, and motif) and approximation of global connectivity of a graph using different prompting methods to enhance LLMs' effectiveness in handling complex graph structures.

63, TITLE: NLTS Hamiltonians and Strongly-Explicit SoS Lower Bounds from Low-Rate Quantum LDPC Codes
AUTHORS: Louis Golowich ; Tali Kaufman
CATEGORY: quant-ph [quant-ph, cs.CC]
HIGHLIGHT: Recent constructions of the first asymptotically good quantum LDPC (qLDPC) codes led to two breakthroughs in complexity theory: the NLTS (No Low-Energy Trivial States) theorem (Anshu, Breuckmann, and Nirkhe, STOC'23), and explicit lower bounds against a linear number of levels of the Sum-of-Squares (SoS) hierarchy (Hopkins and Lin, FOCS'22). In this work, we obtain improvements to both of these results using qLDPC codes of low rate: - Whereas Anshu et al. only obtained NLTS Hamiltonians from qLDPC codes of linear dimension, we show the stronger result that qLDPC codes of arbitrarily small positive dimension yield NLTS Hamiltonians.

64, TITLE: LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks
AUTHORS: Mihir Parmar ; Aakanksha Naik ; Himanshu Gupta ; Disha Agrawal ; Chitta Baral
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: Assessing these models on long sequences is crucial since prior work in the general domain has demonstrated performance degradation of LLMs on longer texts. Motivated by this, we introduce LongBoX, a collection of seven medical datasets in text-to-text format, designed to investigate model performance on long sequences.

65, TITLE: To Be or Not to Be? An Exploration of Continuously Controllable Prompt Engineering
AUTHORS: YUHAN SUN et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In response, we introduce ControlPE (Continuously Controllable Prompt Engineering).

66, TITLE: R-Tuning: Teaching Large Language Models to Refuse Unknown Questions
AUTHORS: HANNING ZHANG et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning).

67, TITLE: ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks
AUTHORS: YULIANG LIU et. al.
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: Instead of evaluating LLMs to code from scratch, this work aims to propose a new evaluation setup where LLMs use open-source libraries to finish machine learning tasks.

68, TITLE: Human Still Wins Over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks
AUTHORS: YUXUAN LU et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we conduct an empirical experiment on four datasets from three different domains comparing SOTA LLMs with small models trained on expert annotations with AL.

69, TITLE: More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering
AUTHORS: BINGSHENG YAO et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to produce the most confident prediction results by optimizing the construction of multiple ICL prompt inputs.

70, TITLE: Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion
AUTHORS: Smriti Singh ; Cornelia Caragea ; Junyi Jessy Li
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: First, we introduce a novel dataset EmoTrigger, consisting of 900 social media posts sourced from three different datasets; these were annotated by experts for emotion triggers with high agreement. Using EmoTrigger, we evaluate the ability of large language models (LLMs) to identify emotion triggers, and conduct a comparative analysis of the features considered important for these tasks between LLMs and fine-tuned models.

71, TITLE: AMRFact: Enhancing Summarization Factuality Evaluation with AMR-driven Training Data Generation
AUTHORS: Haoyi Qiu ; Kung-Hsiang Huang ; Jingnong Qu ; Nanyun Peng
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: However, summaries produced by these approaches are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a novel framework that generates factually inconsistent summaries using Abstract Meaning Representation (AMR).

72, TITLE: MMOE: Mixture of Multimodal Interaction Experts
AUTHORS: Haofei Yu ; Paul Pu Liang ; Ruslan Salakhutdinov ; Louis-Philippe Morency
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Notably, the current methods for capturing shared information often do not extend well to these more nuanced interactions, sometimes performing as low as 50% in binary classification. In this paper, we address this problem via a new approach called MMOE, which stands for a mixture of multimodal interaction experts.

73, TITLE: Improving Fit to Human Reading Times Via Temperature-scaled Surprisal
AUTHORS: Tong Liu ; Iza ?krjanec ; Vera Demberg
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: We propose to use temperature-scaled surprisal, a surprisal calculated by shaped probability, to be the predictor of human reading times.

74, TITLE: Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness
AUTHORS: Ashim Gupta ; Rishanth Rajendhran ; Nathan Stringham ; Vivek Srikumar ; Ana Marasovi?
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Are the longstanding robustness issues in NLP resolved by today's larger and more performant models? To address this question, we conduct a thorough investigation using 19 models of different sizes spanning different architectural choices and pretraining objectives.

75, TITLE: Empirical Evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science
AUTHORS: Sridevi Wagle ; Sai Munikoti ; Anurag Acharya ; Sara Smith ; Sameera Horawalavithana
CATEGORY: cs.CL [cs.CL, cs.AI, I.2.7]
HIGHLIGHT: However, there is limited to no research on UQ for RALMs, particularly in scientific contexts. This study aims to address this gap by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific tasks.

76, TITLE: LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction
AUTHORS: Jamie McCusker
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: Through this analysis and a qualitative analysis of sentence extractions via all methods, we found that LOKE-GPT extractions are of high utility for the KGC task and suitable for use in semi-automated extraction settings.

77, TITLE: Prudent Silence or Foolish Babble? Examining Large Language Models' Responses to The Unknown
AUTHORS: Genglin Liu ; Xingyao Wang ; Lifan Yuan ; Yangyi Chen ; Hao Peng
CATEGORY: cs.CL [cs.CL, cs.AI, cs.LG]
HIGHLIGHT: This behavior misaligns with human conversational norms and presents challenges surrounding responsible and ethical AI development. This work aims to systematically investigate LLMs' behaviors in such situations.

78, TITLE: LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue Systems
AUTHORS: Nalin Kumar ; Ond?ej Du?ek
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we introduce methods for achieving dialogue alignment in a GPT-2-based end-to-end dialogue system through the utilization of shared vocabulary.

79, TITLE: Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation
AUTHORS: ZONGHAI YAO et. al.
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4.

80, TITLE: Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies
AUTHORS: Zihao He ; Siyi Guo ; Ashwin Rao ; Kristina Lerman
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this study, we address the challenge of understanding political bias in digitized discourse using LLMs.

81, TITLE: Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation
AUTHORS: Yifu Qiu ; Varun Embar ; Shay B. Cohen ; Benjamin Han
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: To reduce hallucinations, we propose a novel decoding method, TWEAK (Think While Effectively Articulating Knowledge).

82, TITLE: OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking
AUTHORS: Chia-Hsuan Lee ; Hao Cheng ; Mari Ostendorf
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance.

83, TITLE: Redefining Super-Resolution: Fine-mesh PDE Predictions Without Classical Simulations
AUTHORS: Rajat Kumar Sarkar ; Ritam Majumdar ; Vishal Jadhav ; Sagar Srinivas Sakhinana ; Venkataramana Runkana
CATEGORY: physics.flu-dyn [physics.flu-dyn, cs.AI, cs.LG, physics.comp-ph]
HIGHLIGHT: We propose a novel definition of super-resolution tailored for PDE-based problems.

84, TITLE: Simulating Opinion Dynamics with Networks of LLM-based Agents
AUTHORS: YUN-SHIUAN CHUANG et. al.
CATEGORY: physics.soc-ph [physics.soc-ph, cs.CL]
HIGHLIGHT: We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs).

85, TITLE: On The Pauli Spectrum of QAC0
AUTHORS: Shivam Nadimpalli ; Natalie Parham ; Francisca Vasconcelos ; Henry Yuen
CATEGORY: quant-ph [quant-ph, cs.CC]
HIGHLIGHT: In this work, we identify a notion of the \emph{Pauli spectrum} of $\mathsf{QAC}^0$ circuits, which can be viewed as the quantum analogue of the Fourier spectrum of classical $\mathsf{AC}^0$ circuits.

86, TITLE: Towards Formal Fault Injection for Safety Assessment of Automated Systems
AUTHORS: Ashfaq Farooqui ; Behrooz Sangchoolie
CATEGORY: cs.AI [cs.AI]
HIGHLIGHT: However, the amount of testing required to evaluate the system is rather large and often a problem. This vision paper introduces formal fault injection, a fusion of these two techniques throughout the development lifecycle to enhance the dependability of autonomous systems.

87, TITLE: Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs
AUTHORS: Sen Yang ; Xin Li ; Leyang Cui ; Lidong Bing ; Wai Lam
CATEGORY: cs.AI [cs.AI, cs.CL]
HIGHLIGHT: Though prompting LLMs with various reasoning structures produces reasoning proofs along with answers, these proofs are not ensured to be causal and reliable due to the inherent defects of LLMs. Tracking such deficiencies, we present a neuro-symbolic integration method, in which a neural LLM is used to represent the knowledge of the problem while an LLM-free symbolic solver is adopted to do deliberative reasoning using the knowledge.

88, TITLE: AutoPlanBench: : Automatically Generating Benchmarks for LLM Planners from PDDL
AUTHORS: Katharina Stein ; Alexander Koller
CATEGORY: cs.AI [cs.AI, cs.CL]
HIGHLIGHT: We present a novel method for automatically converting planning benchmarks written in PDDL into textual descriptions and offer a benchmark dataset created with our method.

89, TITLE: Towards Autonomous Hypothesis Verification Via Language Models with Minimal Guidance
AUTHORS: Shiro Takagi ; Ryutaro Yamauchi ; Wataru Kumagai
CATEGORY: cs.AI [cs.AI, cs.HC, cs.LG]
HIGHLIGHT: Research automation efforts usually employ AI as a tool to automate specific tasks within the research process.

90, TITLE: CRISPR: Eliminating Bias Neurons from An Instruction-following Language Model
AUTHORS: Nakyeong Yang ; Taegwan Kang ; Kyomin Jung
CATEGORY: cs.AI [cs.AI, cs.CL, cs.LG]
HIGHLIGHT: In this paper, we introduces a novel bias mitigation method, CRISPR, designed to alleviate instruction-label biases in LLMs.

91, TITLE: Multi-Step Dialogue Workflow Action Prediction
AUTHORS: Ramya Ramakrishnan ; Ethan Elenberg ; Hashan Narangodage ; Ryan McDonald
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: In this paper, we propose the novel problem of multi-step workflow action prediction, in which the system predicts multiple future workflow actions.

92, TITLE: LifeTox: Unveiling Implicit Toxicity in Life Advice
AUTHORS: MINBEOM KIM et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: To this end, we introduce LifeTox, a dataset designed for identifying implicit toxicity within a broad range of advice-seeking scenarios.

93, TITLE: Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach
AUTHORS: Pritom Saha Akash ; Kashob Kumar Roy ; Lucian Popa ; Kevin Chen-Chuan Chang
CATEGORY: cs.CL [cs.CL, cs.LG]
HIGHLIGHT: Additionally, generating detailed long-form answers often entails aggregating knowledge from diverse sources. To address these limitations, we propose an LFQA model with iterative Planning, Retrieval, and Generation.

94, TITLE: Reducing Privacy Risks in Online Self-Disclosures with Language Models
AUTHORS: YAO DOU et. al.
CATEGORY: cs.CL [cs.CL, cs.HC]
HIGHLIGHT: In this paper, we take the initiative to protect the user-side privacy associated with online self-disclosure through identification and abstraction.

95, TITLE: Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models
AUTHORS: Jinyoung Park ; Ameen Patel ; Omar Zia Khan ; Hyunwoo J. Kim ; Joo-Kyung Kim
CATEGORY: cs.CL [cs.CL, cs.AI, cs.LG]
HIGHLIGHT: To address them, we propose a graph-guided CoT prompting method, which guides the LLMs to reach the correct answer with graph representation/verification steps.

96, TITLE: MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification
AUTHORS: Chadi Helwe ; Tom Calamai ; Pierre-Henri Paris ; Chlo� Clavel ; Fabian Suchanek
CATEGORY: cs.CL [cs.CL, cs.AI, cs.LG]
HIGHLIGHT: Automated detection and classification of fallacies, however, remain challenging, mainly because of the innate subjectivity of the task and the need for a comprehensive, unified approach in existing research. Addressing these limitations, our study introduces a novel taxonomy of fallacies that aligns and refines previous classifications, a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity, adapted to precision, recall, and F1-Score metrics.

97, TITLE: HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM
AUTHORS: ZHILIN WANG et. al.
CATEGORY: cs.CL [cs.CL, cs.AI, cs.LG]
HIGHLIGHT: Existing open-source helpfulness preference datasets do not specify what makes some responses more helpful and others less so. Models trained on these datasets can incidentally learn to model dataset artifacts (e.g. preferring longer but unhelpful responses only due to their length).

98, TITLE: $\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of Information-Seeking Dialogue Via Behavioural Fine-Tuning
AUTHORS: EVGENIIA RAZUMOVSKAIA et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: To mitigate the issue and increase faithfulness of information-seeking dialogue systems, we introduce BeInfo, a simple yet effective method that applies behavioural tuning to aid information-seeking dialogue.

99, TITLE: FollowEval: A Multi-Dimensional Benchmark for Assessing The Instruction-Following Capability of Large Language Models
AUTHORS: YIMIN JING et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: However, these benchmarks are limited to a single language and are constructed using automated approaches, which restricts their applicability and the quality of the test examples they contain. To bridge this gap, we introduce the FollowEval benchmark in this paper.

100, TITLE: PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization
AUTHORS: Joseph J. Peper ; Wenzhao Qiu ; Lu Wang
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: To this end, we present PELMS, a pre-trained model that uses objectives based on semantic coherence heuristics and faithfulness constraints with un-labeled multi-document inputs, to promote the generation of concise, fluent, and faithful summaries.

101, TITLE: Overview of The HASOC Subtrack at FIRE 2023: Identification of Tokens Contributing to Explicit Hate in English By Span Detection
AUTHORS: Sarah Masud ; Mohammad Aflah Khan ; Md. Shad Akhtar ; Tanmoy Chakraborty
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: As hate speech continues to proliferate on the web, it is becoming increasingly important to develop computational methods to mitigate it.

102, TITLE: Spoken Word2Vec: A Perspective And Some Techniques
AUTHORS: Mohammad Amaan Sayeed ; Hanan Aldarmaki
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: In this paper, we examine the assumptions and architectures used in previous works and show experimentally how Word2Vec algorithms fail to encode distributional semantics when the input units are acoustically correlated.

103, TITLE: Enchancing Semi-Supervised Learning for Extractive Summarization with An LLM-based Pseudolabeler
AUTHORS: Gaurav Sahu ; Olga Vechtomova ; Issam H. Laradji
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: Specifically, we propose a prompt-based pseudolabel selection strategy using GPT-4.

104, TITLE: SUQL: Conversational Search Over Structured and Unstructured Data with Large Language Models
AUTHORS: SHICHENG LIU et. al.
CATEGORY: cs.CL [cs.CL, cs.PL]
HIGHLIGHT: To validate our approach, we introduce a dataset consisting of crowdsourced questions and conversations about real restaurants.

105, TITLE: Towards Pragmatic Awareness in Question Answering: A Case Study in Maternal and Infant Health
AUTHORS: Neha Srikanth ; Rupak Sarkar ; Rachel Rudinger ; Jordan Boyd-Graber
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In a high-risk domain such as maternal and infant health, a question-answering system must recognize these pragmatic constraints and go beyond simply answering user questions, examining them in context to respond helpfully. To achieve this, we study pragmatic inferences made when mothers ask questions about pregnancy and infant care.

106, TITLE: On Retrieval Augmentation and The Limitations of Language Model Training
AUTHORS: TING-RUI CHIANG et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we first rule out one previously posited possibility -- the "softmax bottleneck."

107, TITLE: Structured Chemistry Reasoning with Large Language Models
AUTHORS: SIRU OUYANG et. al.
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: We propose InstructChem, a new structured reasoning approach that substantially boosts the LLMs' chemical reasoning capabilities.

108, TITLE: GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks
AUTHORS: Shivanshu Gupta ; Clemens Rosenbaum ; Ethan R. Elenberg
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We propose a novel metric, GistScore, based on Example Gisting, a novel approach for training example retrievers for ICL using an attention bottleneck via Gisting, a recent technique for compressing task instructions.

109, TITLE: SCORE: A Framework for Self-Contradictory Reasoning Evaluation
AUTHORS: Ziyi Liu ; Isabelle Lee ; Yongkang Du ; Soumya Sanyal ; Jieyu Zhao
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this paper, we propose a framework \textsc{SCORE} to analyze how well LLMs can reason.

110, TITLE: Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks
AUTHORS: NEGAR MOKHBERIAN et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we propose Annotator Aware Representations for Texts (AART) for subjective classification tasks.

111, TITLE: Work State-Centric AI Agents: Design, Implementation, and Management of Cognitive Work Threads
AUTHORS: Chen Zhang
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: We propose a work state-centric AI agent model employing "work notes" to record and reflect the state throughout task execution.

112, TITLE: Event Causality Is Key to Computational Story Understanding
AUTHORS: Yidan Sun ; Qin Chao ; Boyang Li
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Leveraging recent progress in large language models (LLMs), we present the first method for event causality identification that leads to material improvements in computational story understanding.

113, TITLE: Leveraging LLMs in Scholarly Knowledge Graph Question Answering
AUTHORS: Tilahun Abedissa Taffa ; Ricardo Usbeck
CATEGORY: cs.CL [cs.CL, cs.AI, cs.DB, cs.LG]
HIGHLIGHT: This paper presents a scholarly Knowledge Graph Question Answering (KGQA) that answers bibliographic natural language questions by leveraging a large language model (LLM) in a few-shot manner.

114, TITLE: Language Generation from Human Brain Activities
AUTHORS: ZIYI YE et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Inspired by recent research that revealed associations between the brain and the large computational language models, we propose a generative language BCI that utilizes the capacity of a large language model (LLM) jointly with a semantic brain decoder to directly generate language from functional magnetic resonance imaging (fMRI) input.

115, TITLE: To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages
AUTHORS: Benedikt Ebing ; Goran Glava?
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Given, on one hand, the large body of work on improving XLT with multilingual LMs and, on the other hand, recent advances in massively multilingual MT, in this work, we systematically evaluate existing and propose new translation-based XLT approaches for transfer to low-resource languages.

116, TITLE: ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy and Communication in Pediatric Populations and Beyond
AUTHORS: Kanhai S. Amin ; Linda Mayes ; Pavan Khosla ; Rushabh Doshi
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: We investigate whether large language models (LLMs) can serve as a medium to improve health literacy in children and other populations.

117, TITLE: Tied-Lora: Enhacing Parameter Efficiency of LoRA with Weight Tying
AUTHORS: Adithya Renduchintala ; Tugrul Konuk ; Oleksii Kuchaiev
CATEGORY: cs.CL [cs.CL, cs.AI, cs.LG]
HIGHLIGHT: We propose Tied-LoRA, a simple paradigm utilizes weight tying and selective training to further increase parameter efficiency of the Low-rank adaptation (LoRA) method.

118, TITLE: Lighter, Yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization
AUTHORS: George Chrysostomou ; Zhixue Zhao ; Miles Williams ; Nikolaos Aletras
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: In this paper, we provide an extensive empirical study on the hallucinations produced by pruned models across three standard summarization tasks, two pruning approaches, three instruction-tuned LLMs, and three hallucination evaluation metrics.

119, TITLE: A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection
AUTHORS: Thi-Nhung Nguyen ; Hoang Ngo ; Kiem-Hieu Nguyen ; Tuan-Dung Cao
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: However, aspect representations are limited by the quality of initial seed words, and model performances are compromised by noise. To mitigate this limitation, we propose a simple framework that automatically enhances the quality of initial seed words and selects high-quality sentences for training instead of using the entire dataset.

120, TITLE: GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding
AUTHORS: ANDOR DIERA et. al.
CATEGORY: cs.CL [cs.CL, cs.PL]
HIGHLIGHT: As part of the full dataset, we introduce a new, manually curated subset StatCodeSearch that focuses on R, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science.

121, TITLE: Evaluating In-Context Learning of Libraries for Code Generation
AUTHORS: Arkil Patel ; Siva Reddy ; Dzmitry Bahdanau ; Pradeep Dasigi
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context.

122, TITLE: Online Continual Knowledge Learning for Language Models
AUTHORS: Yuhao Wu ; Tongjun Shi ; Karthick Sharma ; Chun Wei Seah ; Shuhao Zhang
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: In this paper, we introduce a novel problem in the realm of continual learning: Online Continual Knowledge Learning (OCKL).

123, TITLE: Predicting Generalization Performance with Correctness Discriminators
AUTHORS: Yuekun Yao ; Alexander Koller
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We present a novel model that establishes upper and lower bounds on the accuracy, without requiring gold labels for the unseen data.

124, TITLE: GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets
AUTHORS: Wolfgang Otto ; Matth�us Zloch ; Lu Gan ; Saurav Karmakar ; Stefan Dietze
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around ML models and datasets.

125, TITLE: Evolving Domain Adaptation of Pretrained Language Models for Text Classification
AUTHORS: YUN-SHIUAN CHUANG et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Adapting pre-trained language models (PLMs) for time-series text classification amidst evolving domain shifts (EDS) is critical for maintaining accuracy in applications like stance detection.

126, TITLE: Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models
AUTHORS: Yueqing Liang ; Lu Cheng ; Ali Payani ; Kai Shu
CATEGORY: cs.CL [cs.CL, cs.AI, cs.CY, cs.LG]
HIGHLIGHT: In a dynamic and complex digital world, it is crucial to investigate the vulnerabilities of these detection models to adversarial fairness attacks to improve their fairness robustness. We propose a simple yet effective framework FABLE that leverages backdoor attacks as they allow targeted control over the fairness and detection performance.

127, TITLE: LePaRD: A Large-Scale Dataset of Judges Citing Precedents
AUTHORS: Robert Mahari ; Dominik Stammbach ; Elliott Ash ; Alex `Sandy' Pentland
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We present the Legal Passage Retrieval Dataset LePaRD.

128, TITLE: Evaluating LLM Agent Group Dynamics Against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds
AUTHORS: YUN-SHIUAN CHUANG et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: This study investigates the potential of Large Language Models (LLMs) to simulate human group dynamics, particularly within politically charged contexts.

129, TITLE: Fumbling in Babel: An Investigation Into ChatGPT's Language Identification Ability
AUTHORS: Wei-Rui Chen ; Ife Adebara ; Khai Duy Doan ; Qisheng Liao ; Muhammad Abdul-Mageed
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we investigate ChatGPT's language identification abilities.

130, TITLE: A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms
AUTHORS: Swapnil Mane ; Suman Kundu ; Rajesh Sharma
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this paper, we analyzed the diversity of definitions and proposed a unified cyber-aggression definition.

131, TITLE: Personalized Jargon Identification for Enhanced Interdisciplinary Communication
AUTHORS: YUE GUO et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We compare supervised and prompt-based approaches, finding that prompt-based methods including personal publications yields the highest accuracy, though zero-shot prompting provides a strong baseline.

132, TITLE: CARE: Extracting Experimental Findings From Clinical Literature
AUTHORS: Aakanksha Naik ; Bailey Kuehl ; Erin Bransom ; Doug Downey ; Tom Hope
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Focusing on biomedicine, this work presents CARE (Clinical Aggregation-oriented Result Extraction) -- a new IE dataset for the task of extracting clinical findings.

133, TITLE: MOKA: Moral Knowledge Augmentation for Moral Event Extraction
AUTHORS: Xinliang Frederick Zhang ; Winston Wu ; Nick Beauchamp ; Lu Wang
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Moral theories have been applied to news analysis studying moral values in isolation, while the intricate dynamics among participating entities in shaping moral events have been overlooked.

134, TITLE: Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset
AUTHORS: BROOKLYN SHEPPARD et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this paper, we discuss the methodology used, analyze the annotations obtained, and provide baselines using common NLP algorithms in the context of misogyny detection and mitigation.

135, TITLE: Neural Machine Translation for Automated Feedback on Children's Early-stage Writing
AUTHORS: Jonas Vestergaard Jensen ; Mikkel Jordahn ; Michael Riis Andersen
CATEGORY: cs.CL [cs.CL, cs.LG, I.2.7]
HIGHLIGHT: In this work, we address the problem of assessing and constructing feedback for early-stage writing automatically using machine learning.

136, TITLE: Generative AI for Hate Speech Detection: Evaluation and Findings
AUTHORS: Sagi Pendzel ; Tomer Wullach ; Amir Adler ; Einat Minkov
CATEGORY: cs.CL [cs.CL, cs.AI]
HIGHLIGHT: In this chapter, we provide a review of relevant methods, experimental setups and evaluation of this approach.

137, TITLE: FairytaleCQA: Integrating A Commonsense Knowledge Graph Into Children's Storybook Narratives
AUTHORS: JIAJU CHEN et. al.
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We introduce the FairytaleCQA dataset, which is annotated by children education experts, to supplement 278 storybook narratives with educationally appropriate commonsense knowledge.

138, TITLE: How Does Calibration Data Affect The Post-training Pruning and Quantization of Large Language Models?
AUTHORS: Miles Williams ; Nikolaos Aletras
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this paper, we present the first extensive empirical study on the effect of calibration data upon LLM performance.

139, TITLE: Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents
AUTHORS: Quinn Patwardhan ; Grace Hui Yang
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: We leverage several purpose-built Language Models, including BERT, Chat-based, and text-to-transfer-based models, for text understanding, classification, generation, and summarization.

140, TITLE: Translation Aligned Sentence Embeddings for Turkish Language
AUTHORS: Eren Unlu ; Unver Ciftci
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: Due to the limited availability of high quality datasets for training sentence embeddings in Turkish, we propose a training methodology and a regimen to develop a sentence embedding model.

141, TITLE: SegMix: A Simple Structure-Aware Data Augmentation Method
AUTHORS: Yuxin Pei ; Pushkar Bhuse ; Zhengzhong Liu ; Eric Xing
CATEGORY: cs.CL [cs.CL, cs.AI, cs.LG]
HIGHLIGHT: To this end, we propose SegMix, a collection of interpolation-based DA algorithms that can adapt to task-specific structures.

142, TITLE: SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue NLU
AUTHORS: Evgeniia Razumovskaia ; Goran Glava? ; Anna Korhonen ; Ivan Vuli?
CATEGORY: cs.CL [cs.CL]
HIGHLIGHT: In this work, we introduce SQATIN, a new framework for dialog NLU based on (i) instruction tuning and (ii) question-answering-based formulation of ID and VE tasks.
